{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd162037-4cc8-4678-949e-8fca8282bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from time import time\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93c4bfb6-171c-44e9-a1e7-1dbaa8fad64b",
   "metadata": {},
   "outputs": [],
   "source": [
    " engine= create_engine(\"postgresql://ahmed:1234@localhost:5433/green_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e77be31c-56bb-4c7b-b04e-b575dd5cceb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its already here\n",
      "its allready here \n",
      "total_size_befor_load 9527720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz'\n",
    "input_file='green_tripdata_2019-10.csv.gz'\n",
    "output_file = 'output_file.csv'\n",
    "if not os.path.exists(input_file):\n",
    "            os.system(f'wget {url} -O {input_file}' )\n",
    "else:\n",
    "            print(\"its already here\")\n",
    "            \n",
    "if not os.path.exists(output_file):\n",
    "            with gzip.open(input_file,'rb') as f_in:\n",
    "                with open(output_file,'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in,f_out)\n",
    "else:\n",
    "            print(\"its allready here \")\n",
    "                \n",
    "df=pd.read_csv(output_file,low_memory=False)\n",
    "print(f\"total_size_befor_load {df.size}\")\n",
    "        \n",
    "df.to_sql(name='green_taxi',con=engine,if_exists='replace')\n",
    "        # print(pd.io.sql.get_schema(df,name='green_taxi',con=engine))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba5d9109-daff-4690-8ef9-c4f0dc67dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_size_after_load     count\n",
      "0  486386\n",
      "inserted another chunk....,took 2.898\n",
      "total_size_after_load     count\n",
      "0  496386\n",
      "inserted another chunk....,took 3.064\n",
      "total_size_after_load     count\n",
      "0  506386\n",
      "inserted another chunk....,took 2.918\n",
      "total_size_after_load     count\n",
      "0  516386\n",
      "inserted another chunk....,took 3.222\n",
      "total_size_after_load     count\n",
      "0  526386\n",
      "inserted another chunk....,took 3.122\n",
      "total_size_after_load     count\n",
      "0  536386\n",
      "inserted another chunk....,took 3.025\n",
      "total_size_after_load     count\n",
      "0  546386\n",
      "inserted another chunk....,took 3.036\n",
      "total_size_after_load     count\n",
      "0  556386\n",
      "inserted another chunk....,took 2.797\n",
      "total_size_after_load     count\n",
      "0  566386\n",
      "inserted another chunk....,took 2.809\n",
      "total_size_after_load     count\n",
      "0  576386\n",
      "inserted another chunk....,took 2.865\n",
      "total_size_after_load     count\n",
      "0  586386\n",
      "inserted another chunk....,took 3.019\n",
      "total_size_after_load     count\n",
      "0  596386\n",
      "inserted another chunk....,took 2.515\n",
      "total_size_after_load     count\n",
      "0  606386\n",
      "inserted another chunk....,took 3.063\n",
      "total_size_after_load     count\n",
      "0  616386\n",
      "inserted another chunk....,took 2.799\n",
      "total_size_after_load     count\n",
      "0  626386\n",
      "inserted another chunk....,took 2.739\n",
      "total_size_after_load     count\n",
      "0  636386\n",
      "inserted another chunk....,took 2.883\n",
      "total_size_after_load     count\n",
      "0  646386\n",
      "inserted another chunk....,took 2.811\n",
      "total_size_after_load     count\n",
      "0  656386\n",
      "inserted another chunk....,took 3.154\n",
      "total_size_after_load     count\n",
      "0  666386\n",
      "inserted another chunk....,took 2.869\n",
      "total_size_after_load     count\n",
      "0  676386\n",
      "inserted another chunk....,took 4.499\n",
      "total_size_after_load     count\n",
      "0  686386\n",
      "inserted another chunk....,took 3.598\n",
      "total_size_after_load     count\n",
      "0  696386\n",
      "inserted another chunk....,took 3.592\n",
      "total_size_after_load     count\n",
      "0  706386\n",
      "inserted another chunk....,took 3.052\n",
      "total_size_after_load     count\n",
      "0  716386\n",
      "inserted another chunk....,took 2.823\n",
      "total_size_after_load     count\n",
      "0  726386\n",
      "inserted another chunk....,took 3.580\n",
      "total_size_after_load     count\n",
      "0  736386\n",
      "inserted another chunk....,took 3.285\n",
      "total_size_after_load     count\n",
      "0  746386\n",
      "inserted another chunk....,took 3.824\n",
      "total_size_after_load     count\n",
      "0  756386\n",
      "inserted another chunk....,took 4.744\n",
      "total_size_after_load     count\n",
      "0  766386\n",
      "inserted another chunk....,took 4.289\n",
      "total_size_after_load     count\n",
      "0  776386\n",
      "inserted another chunk....,took 5.321\n",
      "total_size_after_load     count\n",
      "0  786386\n",
      "inserted another chunk....,took 3.999\n",
      "total_size_after_load     count\n",
      "0  796386\n",
      "inserted another chunk....,took 4.874\n",
      "total_size_after_load     count\n",
      "0  806386\n",
      "inserted another chunk....,took 3.803\n",
      "total_size_after_load     count\n",
      "0  816386\n",
      "inserted another chunk....,took 3.643\n",
      "total_size_after_load     count\n",
      "0  826386\n",
      "inserted another chunk....,took 5.644\n",
      "total_size_after_load     count\n",
      "0  836386\n",
      "inserted another chunk....,took 4.101\n",
      "total_size_after_load     count\n",
      "0  846386\n",
      "inserted another chunk....,took 4.198\n",
      "total_size_after_load     count\n",
      "0  856386\n",
      "inserted another chunk....,took 3.127\n",
      "total_size_after_load     count\n",
      "0  866386\n",
      "inserted another chunk....,took 3.456\n",
      "total_size_after_load     count\n",
      "0  876386\n",
      "inserted another chunk....,took 2.984\n",
      "total_size_after_load     count\n",
      "0  886386\n",
      "inserted another chunk....,took 2.596\n",
      "total_size_after_load     count\n",
      "0  896386\n",
      "inserted another chunk....,took 3.344\n",
      "total_size_after_load     count\n",
      "0  906386\n",
      "inserted another chunk....,took 3.022\n",
      "total_size_after_load     count\n",
      "0  916386\n",
      "inserted another chunk....,took 2.701\n",
      "total_size_after_load     count\n",
      "0  926386\n",
      "inserted another chunk....,took 3.787\n",
      "total_size_after_load     count\n",
      "0  936386\n",
      "inserted another chunk....,took 4.769\n",
      "total_size_after_load     count\n",
      "0  946386\n",
      "inserted another chunk....,took 4.318\n",
      "total_size_after_load     count\n",
      "0  952772\n",
      "inserted another chunk....,took 3.649\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     start_time\u001b[38;5;241m=\u001b[39mtime()\n\u001b[1;32m----> 5\u001b[0m     next_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_taxi_iterat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     next_data\u001b[38;5;241m.\u001b[39mlpep_pickup_datetime \u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mto_datetime(next_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlpep_pickup_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m     next_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlpep_dropoff_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mto_datetime(next_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlpep_dropoff_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1843\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1845\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1985\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m   1984\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[1;32m-> 1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:863\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "data_taxi_iterat=pd.read_csv(output_file,iterator=True, chunksize=10000)\n",
    "while True:\n",
    "        \n",
    "    start_time=time()\n",
    "    next_data=next(data_taxi_iterat)\n",
    "    next_data.lpep_pickup_datetime =pd.to_datetime(next_data['lpep_pickup_datetime'])\n",
    "    next_data['lpep_dropoff_datetime']=pd.to_datetime(next_data['lpep_dropoff_datetime'])\n",
    "    next_data.to_sql(name='green_taxi',con=engine,if_exists='append')\n",
    "    new_data=pd.read_sql('select count(*) from green_taxi',con=engine)\n",
    "    print(f\"total_size_after_load {new_data}\")\n",
    "    end_time=time()\n",
    "    print('inserted another chunk....,took %.3f'%(end_time-start_time))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c85b39-389c-4cf0-820b-034f1854186b",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0a2ce2a-583a-4d8c-bcd8-b1359bce2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "    try:    \n",
    "        output_zone='taxi_zone_lookup.csv'\n",
    "        zone_url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv'\n",
    "        zone_response = requests.get(zone_url)\n",
    "        zone_response.raise_for_status()  # Crash on HTTP errors\n",
    "\n",
    "        with open('taxi_zone_lookup.csv', 'wb') as f:\n",
    "            f.write(zone_response.content)\n",
    "        data_zone=pd.read_csv(output_zone)\n",
    "        data_zone.to_sql(name='zone',con=engine,if_exists='replace')\n",
    "    except Exception as e:\n",
    "        print(f\"faile t to make zone table {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326970c-0b7f-4c3e-9bba-48bad785c852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f4dc0-b1df-4c19-85a3-b450ea36f3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ffd87-effa-492f-ac05-9c8439aed0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
